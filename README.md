# Reinforcement Learning Optimal Policy Estimation

This repository implements the numerical and data-driven approaches to solve the Short-sighted Reinforcement Learning problem. Specifically, it computes the optimal functions ğœˆ1(ğ‘†) and ğœˆ2(ğ‘†) using numerical methods, and then applies a data-driven approach to obtain approximations ğœ”(ğ‘¢(ğ‘‹,ğœƒ)) using neural networks. The data-driven approach utilizes two sets of pairs, with the functions [A1] and [C1] applied for approximation. For [C1], we demonstrate that the conditional expectations take values in the interval [0,2].


## Problem Overview

The problem involves a Markov decision process with two possible transition densities depending on the action ğ‘

1. For ğ‘ğ‘¡=1: ğ’®ğ‘¡+1=0.8ğ’®ğ‘¡+1.0+ğ‘Šğ‘¡

2. For ğ‘ğ‘¡=2: ğ’®ğ‘¡+1=âˆ’2.0+ğ‘Šğ‘¡
   
Where ğ‘Šğ‘¡âˆ¼ğ‘(0,1).

At each new state, a reward ğ‘…(ğ’®)=ğ‘šğ‘–ğ‘›{2,ğ’®^2} is received. The goal is to solve for the optimal action policy that maximizes the expected reward.

The problem is defined as:
ğœˆğ‘—(ğ‘‹)=ğ”¼ğ‘†ğ‘¡+1ğ‘—[â„›(ğ‘†ğ‘¡+1)|ğ‘†ğ‘¡=ğ‘‹],ğ‘—=1,2

The optimal action policy is:

ğ‘ğ‘¡=ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ {ğœˆ1(ğ‘†ğ‘¡),ğœˆ2(ğ‘†ğ‘¡)}

That is, at time ğ‘¡, choose ğ‘ğ‘¡=1 if ğœˆ1(ğ‘†ğ‘¡)>ğœˆ2(ğ‘†ğ‘¡), otherwise choose ğ‘ğ‘¡=2.

## Methodology

### Numerical Approach

Numerical solutions are computed for the expected rewards using the transition functions and the reward function. For a sampling interval {ğ‘ 0,â€¦,ğ‘ ğ‘›},, the vectors are defined as:

ğ‘‰ğ‘—=[ğœˆğ‘—(ğ‘ 0)â‹®ğœˆğ‘—(ğ‘ ğ‘›)],ğ‘…=[â„›(ğ‘ 0)â‹®â„›(ğ‘ ğ‘›)]

The matrices â„±ğ‘—, for j = 1, 2 are calculated based on the difference in transition probabilities.

â„±ğ‘—1ğ‘˜=0.5(ğ»ğ‘˜(ğ‘ 1|ğ‘ ğ‘—)âˆ’ğ»ğ‘˜(ğ‘ 0|ğ‘ ğ‘—)),ğ‘˜=1,2 

â„±ğ‘—ğ‘›ğ‘˜=0.5(ğ»ğ‘˜(ğ‘ ğ‘›|ğ‘ ğ‘—)âˆ’ğ»ğ‘˜(ğ‘ ğ‘›âˆ’1|ğ‘ ğ‘—)),ğ‘˜=1,2 

â„±ğ‘—ğ‘–ğ‘˜=0.5(ğ»ğ‘˜(ğ‘ ğ‘–|ğ‘ ğ‘—)âˆ’ğ»ğ‘˜(ğ‘ ğ‘–âˆ’2|ğ‘ ğ‘—)),ğ‘˜=1,2,

ğ‘–=2,â€¦,ğ‘›âˆ’1

### Data-Driven Approach

The data-driven approach uses neural networks to approximate ğ‘¢(ğ‘‹,ğœƒğœŠğ‘—). The networks are trained to approximate the expected rewards for each state. The cost function for the neural network is:

ğ½Ì‚(ğœƒğ‘—)=(1/ğ‘›ğ‘—) Î£{ğœ‘(ğ‘¢(ğ‘‹ğ‘–ğ‘—,ğœƒğ‘—))+â„›(ğ‘Œğ‘–ğ‘—)ğœ“(ğ‘¢(ğ‘‹ğ‘–ğ‘—,ğœƒğ‘—))}

where ğ‘¢(ğ‘‹ğ‘–ğ‘—,ğœƒğ‘—) is the neural network approximation.

The optimization algorithm used is Gradient Descent:

ğœƒğ‘¡ğ‘—=ğœƒğ‘¡âˆ’1ğ‘—âˆ’ğœ‡Î£[â„›(ğ‘Œğ‘–ğ‘—)âˆ’ğœ”(ğ‘¢(ğ‘‹ğ‘–ğ‘—,ğœƒğ‘¡âˆ’1ğ‘—))]ğ‘›ğ‘—ğ‘–=1ğœŒ(ğ‘¢(ğ‘‹ğ‘–ğ‘—,ğœƒğ‘¡âˆ’1ğ‘—))âˆ‡ğœƒğ‘¢(ğ‘‹ğ‘–ğ‘—,ğœƒğ‘¡âˆ’1ğ‘—)

### Conditional Expectation Functions

For the data-driven approach, we apply the function families [A1] and [C1]:

- **[A1]**: 
  - ğœ”(ğ“) = ğ“, 
  - ğœŒ(ğ“) = âˆ’1,
  - ğœ‘(ğ“) = z^2 / 2,
  - ğœ“(ğ“) = âˆ’ğ“ 

- **[C1]**:
  - ğœ”(ğ“) = ğ‘ / (1+ğ‘’^ğ“) + ğ‘ / (1+ğ‘’^ğ“),
  - ğœŒ(ğ“) = âˆ’ ğ‘’^ğ“ / (1+ğ‘’^ğ“),
  - ğœ‘(ğ“) = (ğ‘âˆ’ğ‘) / (1 + ğ‘’^ğ“)+ ğ‘ log(1+ğ‘’^ğ“),
  - ğœ“(ğ“) = âˆ’log(1+ğ‘’^ğ“)

### Comparison and Results

After generating 1000 random actions with a 50% chance for each action ğ‘ğ‘¡=1 or ğ‘ğ‘¡=2, the states {ğ‘†1,â€¦,ğ‘†1001} are created. These sets are used to train the neural networks for approximating ğœˆ1(ğ‘‹) and ğœˆ2(ğ‘‹) using the respective data sets.

The neural networks used are of a single hidden layer with 100 neurons and ReLU activation functions. The Gradient Descent learning rate is set to 0.001.

The results demonstrate that the neural network approximations converge well to the numerical solution, with the optimal policy derived from the comparison of ğœˆ1(ğ‘†ğ‘¡) and ğœˆ2(ğ‘†ğ‘¡).

### Results

#### 1. Reward Function
The reward function used in the problem is ğ‘…(ğ’®)=ğ‘šğ‘–ğ‘›{2,ğ’®^2}. Below is a visualization of the reward function across the state space.

   <div style="text-align: center;">
      <img src="screenshots/reward.png" alt="Reward" width="90%" />
   </div>

#### 2. Learning curve of ğ‘¢(ğ‘‹,ğœƒ1) using [A1] and [C1]

The following plots shows the learning curve for ğœˆ1(ğ‘†), computed using the numerical method and the data-driven approach with [A1] and [C1].

   <div style="display: flex; justify-content: space-between; text-align: center; width: 100%;">
      <img src="screenshots/curve1_A1.png" alt="Learning Curve A1" width="45%" />
      <img src="screenshots/curve1_C1.png" alt="Learning Curve C2" width="45%" />
   </div>

#### 3. Learning curve of ğ‘¢(ğ‘‹,ğœƒ2) using [A1] and using [C1]

Similarly, the following plot shows the learning curve for ğœˆ2(ğ‘†), computed using the numerical method and the data-driven approach with [A1] and [C1].

   <div style="display: flex; justify-content: space-between; text-align: center; width: 100%;">
      <img src="screenshots/curve2_A1.png" alt="Learning Curve A1" width="45%" />
      <img src="screenshots/curve2_C1.png" alt="Learning Curve C1" width="45%" />
   </div>

#### 4. Approximations of ğœˆ1(ğ‘†) and ğœˆ2(ğ‘†)

The following plots show the approximations for ğœˆ1(ğ‘†) and ğœˆ2(ğ‘†) obtained from both the numerical method and the neural network (NN) approximation.

- ğœˆ1(ğ‘†) Approximations

   <div style="text-align: center;">
      <img src="screenshots/v1.png" alt="Approximation for V1(X)" width="90%" />
   </div>

- ğœˆ2(ğ‘†) Approximations

   <div style="text-align: center;">
      <img src="screenshots/v2.png" alt="Approximation for V2(X)" width="90%" />
   </div>

#### 5. Comparison of Numerical and Neural Network Approximations of ğœˆ1(ğ‘†) and ğœˆ2(ğ‘†)

This graph shows the comparison between the numerical solutions and the neural network approximations for both ğœˆ1(ğ‘†) and ğœˆ2(ğ‘†). The black curve represents the numerical solution, while the blue and red curves represent the neural network approximations using [A1] and [C1], respectively.

   <div style="text-align: center;">
      <img src="screenshots/both.png" alt="Approximation for both" width="90%" />
   </div>

#### 6. Optimal Action Policy

Finally, the optimal action policy based on the values of ğœˆ1(ğ‘†) and ğœˆ2(ğ‘†) is shown below.  The policy chooses ğ‘ğ‘¡=1 if ğœˆ1(ğ‘†ğ‘¡) > ğœˆ2(ğ‘†ğ‘¡) and ğ‘ğ‘¡=1 otherwise.

   <div style="text-align: center;">
      <img src="screenshots/action_policy.png" alt="Action Policy" width="90%" />
   </div>
### Conclusion

The data-driven approach using neural networks provides good approximations to the optimal functions and action policies, showing that the use of functions [A1] and [C1] successfully estimates the conditional expectations in this reinforcement learning problem.

## Installation

To clone the repository:

        git clone https://github.com/orestis-koutroumpas/Reinforcement-Learning-Optimal-Policy.git

### Dependencies

- Python 3.x
- NumPy
- Matplotlib
- Scipy